{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule and Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = (x + y)^2\n",
    "\n",
    "x = torch.randn(32, requires_grad=True)\n",
    "y = torch.randn(32)\n",
    "\n",
    "t = (x + y)\n",
    "z = t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9022, -1.3994, -3.6113, -0.1243, -2.7523, -0.2463,  1.6660, -3.5591,\n",
       "        -3.9942, -3.9819,  1.6452, -5.6255,  1.4447, -0.3180, -5.8219, -2.0622,\n",
       "         4.0085,  0.0107, -0.7885,  5.2320,  1.9871,  5.0611,  0.0436,  5.3610,\n",
       "        -0.4770,  0.3224,  2.1578, -0.9394,  0.2355,  2.9944,  0.6156, -0.6121])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9022, -1.3994, -3.6113, -0.1243, -2.7523, -0.2463,  1.6660, -3.5591,\n",
       "        -3.9942, -3.9819,  1.6452, -5.6255,  1.4447, -0.3180, -5.8219, -2.0622,\n",
       "         4.0085,  0.0107, -0.7885,  5.2320,  1.9871,  5.0611,  0.0436,  5.3610,\n",
       "        -0.4770,  0.3224,  2.1578, -0.9394,  0.2355,  2.9944,  0.6156, -0.6121],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (x + y) # partial derivative z over x is same as 2(x+y)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x19759f7dc88>, <PowBackward0 at 0x19759f7df60>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad_fn, z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i, j):\n",
    "        result = i + j\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "y2 = y.data\n",
    "y2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.Add2Backward at 0x19759f0eb40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Add2.apply(x, y2)\n",
    "t.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.backward(torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # 덧셈 노드 역전파는 gradient를 그대로 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i, j):\n",
    "        result = i * j\n",
    "        ctx.save_for_backward(i, j)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.saved_tensors\n",
    "        return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "y2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.Mul2Backward at 0x19759f0ec18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Mul2.apply(x, y2)\n",
    "t.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.backward(torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5021, -0.1684, -2.0597, -0.1771, -0.6056, -0.3150,  0.6844, -1.7798,\n",
       "        -1.4690, -1.6745,  0.8560, -2.0431,  0.2906,  0.4620, -1.9062, -0.2509,\n",
       "         0.6938, -0.5153, -0.0825,  1.7021,  1.8963,  1.9593, -0.4827,  1.6668,\n",
       "        -0.8638,  0.4666,  1.7789, -0.2838, -0.1452,  1.3351,  1.0301, -0.0459])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # is same as y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.4899e-01, -5.3129e-01,  2.5405e-01,  1.1490e-01, -7.7055e-01,\n",
       "         1.9188e-01,  1.4856e-01,  2.6861e-04, -5.2811e-01, -3.1647e-01,\n",
       "        -3.3334e-02, -7.6965e-01,  4.3182e-01, -6.2099e-01, -1.0047e+00,\n",
       "        -7.8020e-01,  1.3105e+00,  5.2063e-01, -3.1181e-01,  9.1392e-01,\n",
       "        -9.0276e-01,  5.7119e-01,  5.0452e-01,  1.0137e+00,  6.2528e-01,\n",
       "        -3.0534e-01, -6.9995e-01, -1.8595e-01,  2.6299e-01,  1.6210e-01,\n",
       "        -7.2231e-01, -2.6012e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.grad # is same as x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사과 쇼핑의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = torch.tensor([100.,], requires_grad=True)\n",
    "num = torch.tensor([2.,], requires_grad=True)\n",
    "ctax = torch.tensor([1.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = apple * num\n",
    "price.retain_grad()\n",
    "result = price * ctax\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.2000]),\n",
       " tensor([110.]),\n",
       " tensor([200.]),\n",
       " tensor([1.1000]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.grad, num.grad, ctax.grad, price.grad, result.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사과와 귤 쇼핑의 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = torch.tensor([100.,], requires_grad=True)\n",
    "tangerine = torch.tensor([150.,], requires_grad=True)\n",
    "num_apple = torch.tensor([2.,], requires_grad=True)\n",
    "num_tangerine = torch.tensor([3.,], requires_grad=True)\n",
    "ctax = torch.tensor([1.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_price = apple * num_apple\n",
    "apple_price.retain_grad()\n",
    "\n",
    "tangerine_price = tangerine * num_tangerine\n",
    "tangerine_price.retain_grad()\n",
    "\n",
    "price = apple_price + tangerine_price\n",
    "price.retain_grad()\n",
    "\n",
    "result = price * ctax\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          apple.grad = 2.20\n",
      "      tangerine.grad = 3.30\n",
      "      num_apple.grad = 110.00\n",
      "  num_tangerine.grad = 165.00\n",
      "           ctax.grad = 650.00\n",
      "    apple_price.grad = 1.10\n",
      "tangerine_price.grad = 1.10\n",
      "          price.grad = 1.10\n",
      "         result.grad = 1.00\n"
     ]
    }
   ],
   "source": [
    "items = ['apple', 'tangerine', 'num_apple', 'num_tangerine', 'ctax']\n",
    "items += ['apple_price', 'tangerine_price', 'price', 'result']\n",
    "\n",
    "for item in items:\n",
    "    print(f\"{item:>15s}.grad = {eval(item).grad.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Activations, Affine and Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 2.5405e-01, 1.1490e-01, 0.0000e+00, 1.9188e-01,\n",
       "        1.4856e-01, 2.6861e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        4.3182e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3105e+00, 5.2063e-01,\n",
       "        0.0000e+00, 9.1392e-01, 0.0000e+00, 5.7119e-01, 5.0452e-01, 1.0137e+00,\n",
       "        6.2528e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6299e-01, 1.6210e-01,\n",
       "        0.0000e+00, 0.0000e+00], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = torch.relu(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        x = i.clone()\n",
    "        ctx.save_for_backward(x < 0)\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        cond, = ctx.saved_tensors\n",
    "        grad_output[cond] = 0\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 2.5405e-01, 1.1490e-01, 0.0000e+00, 1.9188e-01,\n",
       "        1.4856e-01, 2.6861e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        4.3182e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3105e+00, 5.2063e-01,\n",
       "        0.0000e+00, 9.1392e-01, 0.0000e+00, 5.7119e-01, 5.0452e-01, 1.0137e+00,\n",
       "        6.2528e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6299e-01, 1.6210e-01,\n",
       "        0.0000e+00, 0.0000e+00], grad_fn=<Relu2Backward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = Relu2.apply(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = 1 / (1 + np.exp(-i))\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result * (1 - result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2791, 0.3702, 0.5632, 0.5287, 0.3164, 0.5478, 0.5371, 0.5001, 0.3710,\n",
       "        0.4215, 0.4917, 0.3166, 0.6063, 0.3496, 0.2680, 0.3143, 0.7876, 0.6273,\n",
       "        0.4227, 0.7138, 0.2885, 0.6390, 0.6235, 0.7337, 0.6514, 0.4243, 0.3318,\n",
       "        0.4536, 0.5654, 0.5404, 0.3269, 0.4353], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = Sigmoid.apply(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2012, 0.2332, 0.2460, 0.2492, 0.2163, 0.2477, 0.2486, 0.2500, 0.2333,\n",
       "        0.2438, 0.2499, 0.2163, 0.2387, 0.2274, 0.1962, 0.2155, 0.1673, 0.2338,\n",
       "        0.2440, 0.2043, 0.2053, 0.2307, 0.2347, 0.1954, 0.2271, 0.2443, 0.2217,\n",
       "        0.2479, 0.2457, 0.2484, 0.2200, 0.2458])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # is same as z * (1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2012, 0.2332, 0.2460, 0.2492, 0.2163, 0.2477, 0.2486, 0.2500, 0.2333,\n",
       "        0.2438, 0.2499, 0.2163, 0.2387, 0.2274, 0.1962, 0.2155, 0.1673, 0.2338,\n",
       "        0.2440, 0.2043, 0.2053, 0.2307, 0.2347, 0.1954, 0.2271, 0.2443, 0.2217,\n",
       "        0.2479, 0.2457, 0.2484, 0.2200, 0.2458], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z * (1-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 128), requires_grad=True)\n",
    "W = torch.randn((128, 10), requires_grad=True)\n",
    "b = torch.randn((10,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = x @ W\n",
    "result = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MmBackward at 0x19759fae6a0>, <AddBackward0 at 0x19759fae8d0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx.grad_fn, result.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx.retain_grad()\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones_like(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        ...,\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # Wx.grad @ W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(Wx.grad @ W.T, x.grad).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.6591,   1.6591,   1.6591,  ...,   1.6591,   1.6591,   1.6591],\n",
       "        [  5.8406,   5.8406,   5.8406,  ...,   5.8406,   5.8406,   5.8406],\n",
       "        [-14.8959, -14.8959, -14.8959,  ..., -14.8959, -14.8959, -14.8959],\n",
       "        ...,\n",
       "        [  3.8534,   3.8534,   3.8534,  ...,   3.8534,   3.8534,   3.8534],\n",
       "        [ -0.2523,  -0.2523,  -0.2523,  ...,  -0.2523,  -0.2523,  -0.2523],\n",
       "        [ 11.3444,  11.3444,  11.3444,  ...,  11.3444,  11.3444,  11.3444]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad # x.T @ Wx.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(x.T @ Wx.grad, W.grad).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad # Wx의 grad와 동일하지만, dim=0으로 summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(Wx.grad, torch.ones_like(result)).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mm2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W):\n",
    "        ctx.save_for_backward(x, W)\n",
    "        result = x @ W\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W = ctx.saved_tensors\n",
    "        return grad_output @ W.T, x.T @ grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "W.grad = None\n",
    "b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = Mm2.apply(x, W)\n",
    "result = Add2.apply(Wx, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones_like(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        ...,\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412],\n",
       "        [ 2.8950,  1.8922, -5.9358,  ..., -1.9769, -4.1586, -2.2412]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.6591,   1.6591,   1.6591,  ...,   1.6591,   1.6591,   1.6591],\n",
       "        [  5.8406,   5.8406,   5.8406,  ...,   5.8406,   5.8406,   5.8406],\n",
       "        [-14.8959, -14.8959, -14.8959,  ..., -14.8959, -14.8959, -14.8959],\n",
       "        ...,\n",
       "        [  3.8534,   3.8534,   3.8534,  ...,   3.8534,   3.8534,   3.8534],\n",
       "        [ -0.2523,  -0.2523,  -0.2523,  ...,  -0.2523,  -0.2523,  -0.2523],\n",
       "        [ 11.3444,  11.3444,  11.3444,  ...,  11.3444,  11.3444,  11.3444]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
