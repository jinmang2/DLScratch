{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule and Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = (x + y)^2\n",
    "\n",
    "x = torch.randn(32, requires_grad=True)\n",
    "y = torch.randn(32)\n",
    "\n",
    "t = (x + y)\n",
    "z = t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2231, -0.8313,  1.5170,  0.2799,  2.9198,  1.9845,  1.1557,  3.0177,\n",
       "        -1.2327, -3.6421,  0.1778, -5.1457,  0.9701,  0.1705, -0.6928,  0.1140,\n",
       "        -2.3534, -0.6161, -0.3250,  4.0826, -0.7653, -3.4585,  0.0458,  2.0011,\n",
       "         0.3645, -1.6698, -2.1550,  2.1985,  3.5416, -1.1568, -0.5758,  0.5724])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2231, -0.8313,  1.5170,  0.2799,  2.9198,  1.9845,  1.1557,  3.0177,\n",
       "        -1.2327, -3.6421,  0.1778, -5.1457,  0.9701,  0.1705, -0.6928,  0.1140,\n",
       "        -2.3534, -0.6161, -0.3250,  4.0826, -0.7653, -3.4585,  0.0458,  2.0011,\n",
       "         0.3645, -1.6698, -2.1550,  2.1985,  3.5416, -1.1568, -0.5758,  0.5724],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (x + y) # partial derivative z over x is same as 2(x+y)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x20d01f077f0>, <PowBackward0 at 0x20d01f076a0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad_fn, z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i, j):\n",
    "        result = i + j\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "y2 = y.data\n",
    "y2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.Add2Backward at 0x20d01e7eb40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Add2.apply(x, y2)\n",
    "t.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.backward(torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # 덧셈 노드 역전파는 gradient를 그대로 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i, j):\n",
    "        result = i * j\n",
    "        ctx.save_for_backward(i, j)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.saved_tensors\n",
    "        return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "y2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.Mul2Backward at 0x20d01e7ec18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Mul2.apply(x, y2)\n",
    "t.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.backward(torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0350, -0.3540,  0.8244, -0.4111,  1.0865,  0.7121,  0.6865,  0.8284,\n",
       "        -0.0700, -1.6359,  0.7256, -0.6371,  0.3203, -0.5709,  0.2887, -0.2612,\n",
       "        -0.6102, -1.4872,  0.7437,  1.3832,  0.8530, -0.2996,  0.1710, -0.7266,\n",
       "         0.4247, -0.5761, -1.4798,  0.9976,  1.4002,  0.1949,  0.8476,  1.2577])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # is same as y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0766, -0.0617, -0.0658,  0.5510,  0.3734,  0.2801, -0.1086,  0.6804,\n",
       "        -0.5463, -0.1851, -0.6367, -1.9357,  0.1647,  0.6562, -0.6351,  0.3182,\n",
       "        -0.5665,  1.1792, -0.9062,  0.6581, -1.2357, -1.4297, -0.1481,  1.7272,\n",
       "        -0.2424, -0.2588,  0.4023,  0.1016,  0.3706, -0.7733, -1.1355, -0.9715])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.grad # is same as x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사과 쇼핑의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = torch.tensor([100.,], requires_grad=True)\n",
    "num = torch.tensor([2.,], requires_grad=True)\n",
    "ctax = torch.tensor([1.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = apple * num\n",
    "price.retain_grad()\n",
    "result = price * ctax\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.2000]),\n",
       " tensor([110.]),\n",
       " tensor([200.]),\n",
       " tensor([1.1000]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.grad, num.grad, ctax.grad, price.grad, result.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사과와 귤 쇼핑의 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = torch.tensor([100.,], requires_grad=True)\n",
    "tangerine = torch.tensor([150.,], requires_grad=True)\n",
    "num_apple = torch.tensor([2.,], requires_grad=True)\n",
    "num_tangerine = torch.tensor([3.,], requires_grad=True)\n",
    "ctax = torch.tensor([1.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_price = apple * num_apple\n",
    "apple_price.retain_grad()\n",
    "\n",
    "tangerine_price = tangerine * num_tangerine\n",
    "tangerine_price.retain_grad()\n",
    "\n",
    "price = apple_price + tangerine_price\n",
    "price.retain_grad()\n",
    "\n",
    "result = price * ctax\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          apple.grad = 2.20\n",
      "      tangerine.grad = 3.30\n",
      "      num_apple.grad = 110.00\n",
      "  num_tangerine.grad = 165.00\n",
      "           ctax.grad = 650.00\n",
      "    apple_price.grad = 1.10\n",
      "tangerine_price.grad = 1.10\n",
      "          price.grad = 1.10\n",
      "         result.grad = 1.00\n"
     ]
    }
   ],
   "source": [
    "items = ['apple', 'tangerine', 'num_apple', 'num_tangerine', 'ctax']\n",
    "items += ['apple_price', 'tangerine_price', 'price', 'result']\n",
    "\n",
    "for item in items:\n",
    "    print(f\"{item:>15s}.grad = {eval(item).grad.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Activations, Affine and Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0766, 0.0000, 0.0000, 0.5510, 0.3734, 0.2801, 0.0000, 0.6804, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1647, 0.6562, 0.0000, 0.3182, 0.0000, 1.1792,\n",
       "        0.0000, 0.6581, 0.0000, 0.0000, 0.0000, 1.7272, 0.0000, 0.0000, 0.4023,\n",
       "        0.1016, 0.3706, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = torch.relu(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        x = i.clone()\n",
    "        ctx.save_for_backward(x < 0)\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        cond, = ctx.saved_tensors\n",
    "        grad_output[cond] = 0\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0766, 0.0000, 0.0000, 0.5510, 0.3734, 0.2801, 0.0000, 0.6804, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1647, 0.6562, 0.0000, 0.3182, 0.0000, 1.1792,\n",
       "        0.0000, 0.6581, 0.0000, 0.0000, 0.0000, 1.7272, 0.0000, 0.0000, 0.4023,\n",
       "        0.1016, 0.3706, 0.0000, 0.0000, 0.0000], grad_fn=<Relu2Backward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = Relu2.apply(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = 1 / (1 + np.exp(-i))\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result * (1 - result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7458, 0.4846, 0.4835, 0.6344, 0.5923, 0.5696, 0.4729, 0.6638, 0.3667,\n",
       "        0.4538, 0.3460, 0.1261, 0.5411, 0.6584, 0.3464, 0.5789, 0.3620, 0.7648,\n",
       "        0.2878, 0.6588, 0.2252, 0.1931, 0.4630, 0.8491, 0.4397, 0.4357, 0.5992,\n",
       "        0.5254, 0.5916, 0.3158, 0.2431, 0.2746], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = Sigmoid.apply(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1896, 0.2498, 0.2497, 0.2319, 0.2415, 0.2452, 0.2493, 0.2232, 0.2322,\n",
       "        0.2479, 0.2263, 0.1102, 0.2483, 0.2249, 0.2264, 0.2438, 0.2310, 0.1799,\n",
       "        0.2050, 0.2248, 0.1745, 0.1558, 0.2486, 0.1282, 0.2464, 0.2459, 0.2402,\n",
       "        0.2494, 0.2416, 0.2161, 0.1840, 0.1992])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # is same as z * (1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1896, 0.2498, 0.2497, 0.2319, 0.2415, 0.2452, 0.2493, 0.2232, 0.2322,\n",
       "        0.2479, 0.2263, 0.1102, 0.2483, 0.2249, 0.2264, 0.2438, 0.2310, 0.1799,\n",
       "        0.2050, 0.2248, 0.1745, 0.1558, 0.2486, 0.1282, 0.2464, 0.2459, 0.2402,\n",
       "        0.2494, 0.2416, 0.2161, 0.1840, 0.1992], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z * (1-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 128), requires_grad=True)\n",
    "W = torch.randn((128, 10), requires_grad=True)\n",
    "b = torch.randn((10,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = x @ W\n",
    "result = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MmBackward at 0x20d01f57400>, <AddBackward0 at 0x20d01f57048>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx.grad_fn, result.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx.retain_grad()\n",
    "result.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones_like(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        ...,\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # Wx.grad @ W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(Wx.grad @ W.T, x.grad).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2547, -2.2547, -2.2547,  ..., -2.2547, -2.2547, -2.2547],\n",
       "        [ 0.5833,  0.5833,  0.5833,  ...,  0.5833,  0.5833,  0.5833],\n",
       "        [ 0.3981,  0.3981,  0.3981,  ...,  0.3981,  0.3981,  0.3981],\n",
       "        ...,\n",
       "        [-0.4636, -0.4636, -0.4636,  ..., -0.4636, -0.4636, -0.4636],\n",
       "        [ 8.7022,  8.7022,  8.7022,  ...,  8.7022,  8.7022,  8.7022],\n",
       "        [10.7459, 10.7459, 10.7459,  ..., 10.7459, 10.7459, 10.7459]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad # x.T @ Wx.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(x.T @ Wx.grad, W.grad).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad # Wx의 grad와 동일하지만, dim=0으로 summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(Wx.grad, torch.ones_like(result)).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mm2(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W):\n",
    "        ctx.save_for_backward(x, W)\n",
    "        result = x @ W\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W = ctx.saved_tensors\n",
    "        return grad_output @ W.T, x.T @ grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None\n",
    "W.grad = None\n",
    "b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = Mm2.apply(x, W)\n",
    "result = Add2.apply(Wx, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward(torch.ones_like(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        ...,\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347],\n",
       "        [ 4.5338,  3.1817, -1.2500,  ...,  2.8115, -1.0819, -2.6347]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2547, -2.2547, -2.2547,  ..., -2.2547, -2.2547, -2.2547],\n",
       "        [ 0.5833,  0.5833,  0.5833,  ...,  0.5833,  0.5833,  0.5833],\n",
       "        [ 0.3981,  0.3981,  0.3981,  ...,  0.3981,  0.3981,  0.3981],\n",
       "        ...,\n",
       "        [-0.4636, -0.4636, -0.4636,  ..., -0.4636, -0.4636, -0.4636],\n",
       "        [ 8.7022,  8.7022,  8.7022,  ...,  8.7022,  8.7022,  8.7022],\n",
       "        [10.7459, 10.7459, 10.7459,  ..., 10.7459, 10.7459, 10.7459]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy(LogSoftmax + NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의할 점!!**\n",
    "\n",
    "```python\n",
    "class CrossEntropyLoss(_WeightedLoss):\n",
    "    \n",
    "    __constants__ = ['ignore_index', 'reduction']\n",
    "    ignore_index: int\n",
    "    \n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        return F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "```\n",
    "\n",
    "```python\n",
    "def cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100,\n",
    "                  reduce=None, reduction='mean'):\n",
    "    # if not torch.jit.is_scripting() 부분 패스\n",
    "    # size_average, reduce 중 하나가 None일 때 reduction setting\n",
    "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
    "```\n",
    "\n",
    "- Cross Entropy = LogSoftMax + Negative Log Likelihood\n",
    "- 코드로 보면 알겠지만, softmax function 값을 취할 때 dim=1로 들어감.\n",
    "- 이 부분 기억해서 코드짤 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.randn(32, 10, requires_grad=True)\n",
    "target = torch.LongTensor(32,).random_(0, 10)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6035, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = torch.log(torch.softmax(output, dim=1)) # LogSoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6035, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.mean((torch.diag(log_prob[:, target]))) # NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6035, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss()(log_prob, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 구현 차후"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "- https://pytorch.org/docs/master/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "    \n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "    \n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "\n",
    "input = (torch.randn(20,20, dtype=torch.double, requires_grad=True), \n",
    "         torch.randn(30,20, dtype=torch.double, requires_grad=True))\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        ctx.set_materilize_grad(False)\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "        return grad_output * ctx.constant, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
